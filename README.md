
# AI Prompting for Threat Hunting â€” Field Guide

![AI Prompting Banner](assets/banner.png)
This repository is designed as a companion field-guide for SOC analysts and detection engineers looking to harness large-language models (LLMs) for threat hunting. Youâ€™ll find end-to-end prompt frameworks, ready-to-use templates, and example workflows across Windows, Azure, AWS and Elastic.
> **Status:** v0.4 (examples + graphic integration)  
> **Maintainer:** Juan Rivera (juansasoc)  
> **Scope:** Practical prompt patterns, reusable templates, and examples to accelerate threat hunting and detection engineering with LLMs.

---

## ğŸ¯ Goals
- Speed up **hypothesis generation**, **query drafting** (KQL / ESQL / Sigma), and **triage**.
- Provide **safe-by-default** prompt designs that reduce hallucinations and enforce verifiability.
- Offer ready-to-run **copy/paste templates** and an evaluation rubric.

---

## ğŸ§± Principles
1. **Evidence > Eloquence**: require explicit assumptions, inputs, and references.
2. **Schema-first**: always define the data source and fields before asking for analytics.
3. **Reproducible output**: prefer structured JSON or fenced code blocks with the correct language (e.g., `kql`, `esql`, `yaml`).
4. **Guardrails**: ask the model to state uncertainty, list missing context, and propose validation steps.
5. **Least-privilege data**: redact or hash sensitive data; avoid sending raw PII or secrets.

---

## ğŸ“¦ Repository Layout
```
AI-Prompting-for-Threat-Hunting/
â”œâ”€ README.md
â”œâ”€ assets/
â”‚  â””â”€ banner.png
â”œâ”€ prompts/
â”‚  â”œâ”€ foundation-prompts.md
â”‚  â”œâ”€ windows.md
â”‚  â”œâ”€ cloud-azure.md
â”‚  â”œâ”€ cloud-aws.md
â”‚  â”œâ”€ elastic-esql.md
â”‚  â”œâ”€ sigma.yaml
â”‚  â””â”€ triage-and-reports.md
â”œâ”€ examples/
â”‚  â”œâ”€ windows-persistence.md
â”‚  â”œâ”€ windows-defense-evasion.md
â”‚  â”œâ”€ aws-iam-anomalies.md
â”‚  â”œâ”€ aws-guardduty-suspicious-ip.md
â”‚  â”œâ”€ azure-signin-risk.md
â”‚  â”œâ”€ azure-keyvault-access.md
â”‚  â”œâ”€ elastic-esql-lateral-movement.md
â”‚  â””â”€ elastic-esql-injection.md
â”œâ”€ checklists/
â”‚  â”œâ”€ prompt-eval-rubric.md
â”‚  â””â”€ data-sanity-check.md
â””â”€ LICENSE
```

---

## ğŸš€ Quick Start (System + Context)
**System (paste at start of session):**
```
You are an assistant for THREAT HUNTING & DETECTION ENGINEERING. Follow these rules:
- Ask for missing context first (log source, schema, timeframe, environment).
- Be explicit about uncertainty; never invent fields or events.
- Output code in fenced blocks with the correct language (kql, esql, yaml).
- For each answer provide: (1) assumptions, (2) validation steps, (3) alternatives.
- Prefer minimal, testable queries. Optimize only after a working baseline.
```

**Context preamble (fill these):**
```
Platform: <Windows / Elastic / Sentinel / AWS>
Data source(s): <e.g., Sysmon v13, CloudTrail, AzureSignInLogs>
Timeframe: <e.g., last 24h>
Schema: <list the fields and types you actually have>
Objective: <e.g., detect suspicious PowerShell download cradle>
```

---

## ğŸ§° Prompt Templates (Core)
- Hypothesis Generation
- Schema Extraction (from sample logs)
- KQL/ES|QL Draft from Description
- Sigma Rule Scaffold
- False Positive Tuning
- Hunt Debrief / Executive Summary

See `prompts/` for complete text.

---

## ğŸ”¬ Example Flows (1â€“3 per platform)
- **Windows:** PowerShell downloads; Registry Run Keys; Disable security tools
- **Azure:** Risky sign-ins; Key Vault anomalies; Suspicious role assignments
- **AWS:** Unusual IAM activity; GuardDuty repeated findings; S3 public ACLs
- **Elastic/ESQL:** WinRM lateral movement; Index injection; External command execution

See `examples/` for working drafts.

---

## ğŸ“˜ Glossary â€” AI / LLM Terms for Threat Hunters

| Term | What it means / Why it matters |
|---|---|
| **LLM (Large Language Model)** | A model that generates text. Use it to draft hunts, queries, and summariesâ€”but always verify with data. |
| **Prompt Engineering** | Structuring instructions to get reliable, reproducible outputs (queries, hypotheses, reports). |
| **System Prompt** | The persistent â€œrole/rulesâ€ header that sets behavior (guardrails, formats, do/donâ€™t). |
| **Chain-of-Thought (CoT)** | Step-by-step reasoning. Useful for transparent detection logic; keep summaries concise in final reports. |
| **Context Window** | How much text the model can consider at once. Budget tokens; include only essential fields/logs. |
| **Grounding** | Tie outputs to actual sources (field names, sample events, ATT&CK). Reduces hallucinations. |
| **Hallucination** | Confident but wrong output. Mitigate with schema-first prompts, explicit uncertainty, validation steps. |
| **Temperature** | Randomness control. Low = deterministic (best for queries/detections); high = creative (drafting summaries). |
| **Zero-Shot / Few-Shot** | No examples vs. a few examples in the prompt. Few-shot helps enforce style/format. |
| **RAG (Retrieval-Augmented Generation)** | Pull relevant docs/logs first, then prompt. Great for TI notes, playbooks, runbooks. |
| **Embedding** | Vector representations for similarity search; useful for alert dedup/related-case lookups. |
| **Fine-Tuning** | Train a model on your SOC data to match style or domain. Use sparingly; maintenance overhead. |
| **Guardrails** | Constraints: schema only, cite uncertainty, output JSON/code blocks, list validation steps. |
| **Self-Consistency** | Run reasoning multiple times and take consensus to reduce errors for tricky analyses. |
| **Token** | Billing/length unit. Roughly ~4 chars each; matters for large logs or multi-step prompts. |
| **Inference** | Generating an answer from the model. Latency/cost considerations for automation. |
| **Context Injection** | Supplying schemas, field lists, or sample events to ground outputs. |
| **Validation Step** | Explicit instructions on how you (or the model) will verify results before action. |


---

## ğŸ§ª Prompt Evaluation Rubric
Score 0â€“3 across: Grounding, Specificity, Uncertainty, Validation, Portability.  
â‰¥12 = strong â€¢ 9â€“11 = revise â€¢ â‰¤8 = rework

---

## ğŸš§ Risk & Safety Notes
- Never include secrets or raw customer data in prompts.
- Redact identifiers; keep audit logs of prompt inputs.
- Validate all AI-generated queries before use.

---

## ğŸ“‘ License
MIT
